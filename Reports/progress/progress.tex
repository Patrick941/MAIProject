\documentclass[12pt]{extarticle}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{pgfgantt}

\begin{document}

\doublespacing

\title{\textbf{Debugging Problems}}
\author{Patrick Farmer\\ Supervisor: Dr. Jonathan Dukes}
\date{\today}
\maketitle

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{Images/Trinity_Main_Logo.jpg}
\label{fig:logo}
\end{figure}

\begin{center}
\large A Progress Report submitted in partial fulfillment of the requirements for the degree of MAI in Computer Engineering.
\end{center}

\newpage
\title {\Huge \textbf{\textcolor{blue}{Declaration}}}

\vspace{0.5cm}
\small I hereby declare that this Progress Report is entirely my own work and that it has not been submitted as an exercise for a degree at this or any other university.

\vspace{0.5cm}
\small I have read and I understand the plagiarism provisions in the General Regulations of the University Calendar for the current year, found at http://www.tcd.ie/calendar.
\vspace{0.5cm}

\small I have completed the Online Tutorial on avoiding plagiarism `Ready Steady Write', located at http://tcd-ie.libguides.com/plagiarism/ready-steady-write.
\vspace{0.5cm}

\small I consent / do not consent to the examiner retaining a copy of the thesis beyond the examining period, should they so wish (EU GDPR May 2018).
\vspace{0.5cm}

\small I agree that this thesis will not be publicly available, but will be available to TCD staff and students in the University’s open access institutional repository on the Trinity domain only, subject to Irish Copyright Legislation and Trinity College Library conditions of use and acknowledgement.  \textbf{Please consult with your supervisor on this last item before agreeing, and delete if you do not consent}
\vspace{2cm}

\small Signed:~Patrick Farmer\hfill Date:~\today

\newpage
\tableofcontents

\newpage
\section{Introduction}

Due to the developments in LLM capabilities over the past few years have introduced a couple of issues with the classic approach for teaching program. The first of which is the issue of plagiarism from students using AI generated code. Tasks that are often given to novice programmers can be completed within moments by chat GPT so a lot of students will choose this easier alternative.\\
There is then on the other hand the issue that AI assisted programming will likely exist in every job in the coming years, meaning that being able to work effectively with AI is also a very valuable skill. This project aims to address both of these issues by creating a tool that can generate defective code which the student must then debug.\\
This solves the issue of plagiarism as AI will struggle a lot more to fix broken code than it will to generate it.\\
It also teaches the student how to debug which is the most important tool when working with AI generated code.

\section{Goals and Objectives}

\subsection{Primary Goal}

The primary goal of this project is to create a tool that can generate code with bugs in it that provide meaningful lessons for the students that are debugging it.

\subsection{Secondary Goals}

Some more specific goals for this project are:
\begin{itemize}
    \item Have the tool generate varying types of programs and types of bugs so that no two students get the same problem and if the student were to run the tool a number of times they would keep getting different problems.
    \item To the greatest extent possible prevent or detect and correct hallucinations from the model whether that be related to the intiial code creation or the bug injection.
    \item To have the ability to generate test cases for the working code to check if the student has fixed the bug.
    \item Attempt the bug insertion using a more complex approach using abstract syntax trees.
\end{itemize}

\section{Literature Review}

Debugging has always been a crucial part of programming but many universities do not actually directly teach it. This is a problem outlined in Towards a Framework for Teaching Debugging \cite{li2019}. Also as previously mentioned the need for debugging skills is much more important now that AI is becoming more prevalent in the workplace as discussed in A New Programming Exercise for the Generative AI Era \cite{denny2024}. The skill of debugging also has a number of sub-domains that need to be taught too as outlined in \cite{li2019} the key 3 are knowledge of the language, knowledge of the specific program being debugged and knowledge of how to debug. The latter is the skill that this project aims to teach.\\
\\
When novice programmers write code they will inevitably write bugs into their code. The approach they take to solve the bug though is almost always to make small changes to syntax and run the code again straight away and do not take the time to understand the problem as is highlighted in Methods and Tools for Exploring Novice Compilation Behaviour \cite{jadud2006}. This is another issue the tool helps to solve, by injecting bugs that create underlying problems it will force the student to try to understand the code.\\
\\
Another aspect of the debugging process that this tool aims to teach is the use of debugging tools such as python debugger. This is mentioned as one of the sub-domains of debugging in \cite{li2019}, developing the skills to use these debuggers will inevitably help the student debug in the future. However, The Debugging Mindset \cite{odell2017} disagrees and believes that the best way to teach debugging is to force the student to think about the program without any help from tools. The choice of whether to include debugging software can be used is in my opinion best left to the professor who knows what they want to teach the students with the task they are giving.\\
\\
As mentioned in the paper How Beginning Programmers and Code LLMs (Mis)read Each Other \cite{nguyen2024} there is a lot of potential for misunderstanding between the AI and the user, this is something particularly true for novice programmers who will have more difficulty being precise with their wording as they will not understand the problem as well. This is where there is potential for teaching students to use LLMs effectively. I hope to include an optional feature of solving the problem using an LLM that can be turned on or off by the professor depending on the difficulty of the task. A similar approach for giving students access to LLMs is discussed in Promptly: Using Prompt Problems to Teach Learners How to Effectively Utilize AI Code Generators \cite{denny2023}, where it was showed that students who were limited to only using LLMs and no ability to manually change code learned how to prompt the LLM correctly.\\


\section{Current state of the project}

Through the early stages of the project it quickly became apparent that although clear progress was being made there was no metrics to measure this progress. Due to the nature of the output it is also a little difficult to measure the quality of the output. However, a couple of metrics were selected and used to measure to progress of the project and the impact of the features added. These metrics were:
\begin{itemize}
    \item The cognitive complexity of the code generated. This was measured using the cognitive complexity metric from radon.
    \item The cyclomatic complexity of the code generated. This was also measured using radon.
    \item The retry count for the generation of working code.
    \item The retry count for the generation of the bug.
    \item The similarity of the generated code to the rest of the programs generated.
\end{itemize}
There is no necessary ideal for the complexity. That would depend on the target for the task but it is a clear indicator of whether the code is overly simplistic or massively complex. The retry count and similarity score are all better the lower that they are. Below is a graph of the difference in the metrics as features are added to the tool and the impact of each can be seen.\\
\\
\textit{It will take a while to compile all of the saved results and get a graph but I will have the graph and a short discussion before the due date (24th Jan)}\\
\\
There is also a pipeline setup for the benchmarking of the project that is run on a schedule via a cron job through github workflows. This allows for the project to be benchmarked regularly and the results to be stored as artifacts. The code is then run on my local machine to avoid the inevitably excessive API costs that would be incurred running the full benchmarking suite through the openAI API. There is however support for the openAI API which can be used with the flick of a switch in the command line arguments of the batch file.\\
\\
Currently the program is able to generate working code using an LLM, create one default test case, inject a bug into the code also using an LLM and ensure that the test case fails. It is the plan in the future to enable some more complex bug injection using Abstract Syntax Trees but that is not yet implemented, that is one of the significant goals for the second half of the year. Each query to the model will self reflect on itself to improve accuracy. There is also some more targeted manual checks such as making sure that the test case fails when the bug is inserted.


\section{Project Management}
\subsection{Risk Analysis}
There is few risks in the project as it is already in a working state just is not yet in the most mature possible state. One potential risk is the inability to add the more complex bug injection using ASTs. This is a risk because as the graph above shows the LLM has some difficulties performing the bug insertion and it would greatly increase the customisability of the bugs and the runtime of the program as it would not have to retry the bug insertion as many times. 

\subsection{Gantt Chart}
Below is a gantt chart for the project where the workload is seperated into weeks. There final stage is optional and would create a better GUI for demonstration but the essential features are the backend features outlined in the first two phases. There is also a few weeks left at the end to leave space for any unexpected issues that may arise.

\begin{figure}[h!]
\centering
\begin{ganttchart}[
    y unit title=0.5cm,
    y unit chart=0.7cm,
    vgrid,
    hgrid,
    title height=1,
    title/.style={fill=none},
    title label font=\bfseries\footnotesize,
    title label anchor/.style={below=-1.6ex},
    bar/.style={fill=blue!50},
    bar height=0.6,
    group right shift=0,
    group top shift=0.7,
    group height=.3,
    group peaks height=.2,
    x unit=0.8cm
]{1}{12}
    \gantttitle{2025}{12} \\
    \gantttitlelist{1,...,12}{1} \\
    \ganttgroup{Necessary Improvements}{1}{2} \\
    \ganttbar{Self Reflection}{1}{1} \\
    \ganttbar{Code Comparison}{1}{1} \\
    \ganttbar{More test case coverage}{2}{2} \\
    \ganttgroup{AST Implementation}{3}{7} \\
    \ganttbar{Insert any error}{3}{3} \\
    \ganttbar{Reasonable syntax error}{4}{4} \\
    \ganttbar{Underlying logic error}{5}{7} \\
    \ganttgroup{Interactable GUI (optional)}{8}{9} \\
    \ganttbar{Run code in GUI}{8}{8} \\
    \ganttbar{Run debug in GUI}{8}{8} \\
    \ganttbar{LLM sidepanel}{9}{9} \\
    \ganttbar{Default problems + manual panel}{9}{9} \\
\end{ganttchart}
\caption{Gantt Chart for Project Timeline}
\label{fig:gantt}
\end{figure}

\newpage
\section{Conclusion}

So far a lot of progress has been made in creating a functional tool that meets the project outline of generating problematic code that students can solve to improve their skills at debugging. There is definitely room for improvement though with both the metrics used to measure the progress and the features that are yet to be implemented. There is also scope for improvement for the presentation of the tool which I hope to address with the outlined features for GUI if time permits.\\
The tool can only do so much to address the teaching of debugging. The user will still need to setup the tool in a way that will teach the students meaningful lessons. This is why I also hope to create a set of default prompts with the tool that can be used to generate beneficial problems quickly.

\section{References}
\begin{thebibliography}{9}

\bibitem{jadud2006}
Jadud, M. C. (2006). Methods and tools for exploring novice compilation behaviour. Proceedings of the Second International Workshop on Computing Education Research, 73–84. https://doi.org/10.1145/1151588.1151600

\bibitem{li2019}
Li, C., Chan, E., Denny, P., Luxton-Reilly, A., \& Tempero, E. (2019). Towards a Framework for Teaching Debugging. Proceedings of the Twenty-First Australasian Computing Education Conference, 79–86. https://doi.org/10.1145/3286960.3286970

\bibitem{odell2017}
O’Dell, D. H. (2017). The Debugging Mindset: Understanding the psychology of learning strategies leads to effective problem-solving skills. Queue, 15(1), 71–90. https://doi.org/10.1145/3055301.3068754

\bibitem{parkinson2024}
Parkinson, M. M., Hermans, S., Gijbels, D., \& Dinsmore, D. L. (2024). Exploring debugging processes and regulation strategies during collaborative coding tasks among elementary and secondary students. Computer Science Education, 0(0), 1–28. https://doi.org/10.1080/08993408.2024.2305026

\bibitem{whalley2021}
Whalley, J., Settle, A., \& Luxton-Reilly, A. (2021). Analysis of a Process for Introductory Debugging. Proceedings of the 23rd Australasian Computing Education Conference, 11–20. https://doi.org/10.1145/3441636.3442300

\bibitem{whalley2023}
Whalley, J., Settle, A., \& Luxton-Reilly, A. (2023). A Think-Aloud Study of Novice Debugging. ACM Transactions on Computing Education, 23(2), 1–38. https://doi.org/10.1145/3589004

\bibitem{denny2023}
Denny, P., Leinonen, J., Prather, J., Luxton-Reilly, A., Amarouche, T., Becker, B. A., \& Reeves, B. N. (2023). Promptly: Using Prompt Problems to Teach Learners How to Effectively Utilize AI Code Generators. https://doi.org/10.48550/ARXIV.2307.16364

\bibitem{denny2024}
Denny, P., Leinonen, J., Prather, J., Luxton-Reilly, A., Amarouche, T., Becker, B. A., \& Reeves, B. N. (2024). Prompt Problems: A New Programming Exercise for the Generative AI Era. Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1, 296–302. https://doi.org/10.1145/3626252.3630909

\bibitem{nguyen2024}
Nguyen, S., Babe, H. M., Zi, Y., Guha, A., Anderson, C. J., \& Feldman, M. Q. (2024). How Beginning Programmers and Code LLMs (Mis)read Each Other. Proceedings of the CHI Conference on Human Factors in Computing Systems, 1–26. https://doi.org/10.1145/3613904.3642706

\end{thebibliography}

\end{document}